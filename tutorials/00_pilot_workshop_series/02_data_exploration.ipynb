{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79cc8363",
   "metadata": {},
   "source": [
    "<p align=\"left\">\n",
    "  <img src=\"https://img.shields.io/badge/Research%20Mode-ON-4cbb17?style=for-the-badge\" alt=\"Research Mode\">\n",
    "</p>\n",
    "\n",
    "# 02 Â· Data Exploration â€” ASAP CRN Learning Lab  \n",
    "*A guided launchpad for your second ASAP-CRN workspace adventure.*\n",
    "\n",
    "Welcome to the **ASAP-CRN Learning Lab Pilot Workshop Series!**  \n",
    "\n",
    "This notebook walks you through the essentials of data inspection and preliminary analyses in **Verily Workbench**.\n",
    "\n",
    "> ðŸ’¡ **Tip:** Run each cell in order for the smoothest setup experience.  \n",
    "> You can always come back later to experiment and make it your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7edfd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amaraalexander/miniconda3/envs/ASAP-CRN/bin/python\n"
     ]
    }
   ],
   "source": [
    "# setting up environment\n",
    "import sys\n",
    "print(sys.executable)\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1200)\n",
    "\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    import scanpy as sc\n",
    "except ImportError as e:\n",
    "    print(\"Error -> \", e)\n",
    "    print(\"Installing scanpy\")\n",
    "    !conda install scanpy\n",
    "    import scanpy as sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f6f90",
   "metadata": {},
   "source": [
    "## 1. Build Dataset Paths\n",
    "\n",
    "Next, we define the path to the dataset of interest.  \n",
    "In this example, we are working with the **PMDBS singleâ€‘cell RNAâ€‘seq cohort** dataset:\n",
    "\n",
    "- **Workflow** â†’ `pmdbs_sc_rnaseq`  \n",
    "- **Team** â†’ `cohort`  \n",
    "- **Source** â†’ `pmdbs`  \n",
    "- **Type** â†’ `sc-rnaseq`  \n",
    "\n",
    "These components are combined to construct the bucket and dataset names.  \n",
    "We then set the path to the **cohort analysis outputs** and preview the available files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ddd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set general folder paths\n",
    "HOME = Path.home()\n",
    "WS_ROOT = HOME / \"workspace\"\n",
    "DATA_DIR = WS_ROOT / \"Data\"\n",
    "WS_FILES = WS_ROOT / \"ws_files\"\n",
    "\n",
    "if not WS_ROOT.exists():\n",
    "    print(f\"{WS_ROOT} doesn't exist. We need to remount our resources\")\n",
    "    !wb resource mount    \n",
    "\n",
    "print(\"Home directory:     \", HOME)\n",
    "print(\"Workspace root:     \", WS_ROOT)\n",
    "print(\"Data directory:     \", DATA_DIR)\n",
    "print(\"ws_files directory: \", WS_FILES)\n",
    "\n",
    "print(\"\\nContents of workspace root:\")\n",
    "for p in WS_ROOT.glob(\"*\"):\n",
    "    print(\" -\", p.name, \"/\" if p.is_dir() else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a187d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build and set path to desired dataset\n",
    "\n",
    "DATASETS_PATH = WS_ROOT / \"01_PMDBS_scRNAseq\"\n",
    "\n",
    "workflow       = \"pmdbs_sc_rnaseq\"\n",
    "dataset_team   = \"cohort\"\n",
    "dataset_source = \"pmdbs\"\n",
    "dataset_type   = \"sc-rnaseq\"\n",
    "\n",
    "bucket_name  = f\"asap-curated-{dataset_team}-{dataset_source}-{dataset_type}\"\n",
    "dataset_name = f\"asap-{dataset_team}-{dataset_source}-{dataset_type}\"\n",
    "\n",
    "dataset_path = DATASETS_PATH / bucket_name / workflow\n",
    "print(\"Dataset Path:\", dataset_path)\n",
    "cohort_analysis_path = dataset_path / \"cohort_analysis\"\n",
    "\n",
    "!ls  {cohort_analysis_path} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308f61e3",
   "metadata": {},
   "source": [
    "## 2. Metadata Resources\n",
    "\n",
    "Alongside the dataset, we also define a path to the **release metadata resources**.  \n",
    "This folder contains tables describing samples, subjects, brain regions, and experimental conditions.  \n",
    "Previewing the contents helps us confirm which metadata files are available for integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292afbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define metadata folder path\n",
    "ds_metadata_path = WS_ROOT / \"release_resources/cohort-pmdbs-sc-rnaseq/metadata\"\n",
    "\n",
    "#preview contents\n",
    "!ls {ds_metadata_path} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9106f9c7",
   "metadata": {},
   "source": [
    "## 3. Create Local Output Directory\n",
    "\n",
    "To keep our work organized, we create a local directory inside `ws_files` called `pilot_workshop_files`.  \n",
    "This is where weâ€™ll save any outputs (plots, tables, subsetted data) that we want to retain or share.  \n",
    "If the directory doesnâ€™t exist yet, we create it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a local path for workshop files\n",
    "local_data_path = WS_FILES / \"pilot_workshop_files\"\n",
    "\n",
    "# Create the directory if it doesn't already exist\n",
    "if not local_data_path.exists():\n",
    "    local_data_path.mkdir(parents=True)\n",
    "\n",
    "print(f\"Local data directory ready at: {local_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633bb2fb",
   "metadata": {},
   "source": [
    "## 4. Load Data\n",
    "\n",
    "We now bring in the curated dataset files:\n",
    "\n",
    "- **`asap-cohort.final_metadata.csv`** â†’ cellâ€‘level metadata table\n",
    "- **`asap-cohort.final.h5ad`** â†’ full AnnData object containing expression data and annotations  \n",
    "\n",
    "We copy these files into our local `pilot_workshop_files` directory (if not already present) and load them into memory.  \n",
    "The metadata CSV is read into a Pandas dataframe, while the `.h5ad` file is loaded as an AnnData object in backed mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the expected local path\n",
    "cell_metadata_local_path = local_data_path / f\"asap-{dataset_team}.final_metadata.csv\"\n",
    "if not cell_metadata_local_path.exists():\n",
    "    cell_metadata_og_path = cohort_analysis_path / f\"asap-{dataset_team}.final_metadata.csv\"\n",
    "    !cp {cell_metadata_og_path} {cell_metadata_local_path}\n",
    "\n",
    "# load the adata object\n",
    "cell_metadata_df = pd.read_csv(cell_metadata_local_path, low_memory=False)\n",
    "print(f\"We have loaded the cell_metadata for N={cell_metadata_df.shape[0]} cells\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fae7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adata_local_path = local_data_path / f\"asap-{dataset_team}.final.h5ad\"\n",
    "\n",
    "# Check if the adata file already exists locally.\n",
    "if not adata_local_path.exists():\n",
    "    adata_cell_metadata_og_path = cohort_analysis_path / f\"asap-{dataset_team}.final.h5ad\"\n",
    "    !cp {adata_cell_metadata_og_path} {adata_local_path}\n",
    "\n",
    "adata = sc.read_h5ad(adata_local_path, backed=\"r\")\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c76199",
   "metadata": {},
   "source": [
    "## 5. Prepare Data\n",
    "\n",
    "With both metadata tables and anndata loaded, we can begin exploring the dataset. A key step is **merging datasetâ€‘level metadata into cellâ€‘level metadata**. This allows us to annotate each cell with experimental conditions and subject information, enabling richer analyses.\n",
    "\n",
    "Specifically, we combine:\n",
    "- **Sampleâ€‘level metadata** (`SAMPLE.csv`)  \n",
    "- **Subjectâ€‘level metadata** (`SUBJECT.csv`)  \n",
    "- **Brain sample metadata** (`PMDBS.csv`)  \n",
    "- **Experimental condition metadata** (`CONDITION.csv`)  \n",
    "\n",
    "From each table, we select only the relevant columns (IDs, demographics, brain regions, conditions) to keep the merged metadata concise and focused. This merged metadata will later allow us to subset the dataset (e.g., by diagnosis or brain region) and encode Parkinsonâ€™s disease state for downstream analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41c243",
   "metadata": {},
   "source": [
    "### 5.1 Load Metadata Tables & Select Columns\n",
    "\n",
    "To keep the metadata compact and analysis-ready, we select only the fields needed for:\n",
    "\n",
    "- identifying samples and subjects\n",
    "- demographic variables\n",
    "- brain region information\n",
    "- condition or diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1515b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample-level metadata\n",
    "SAMPLE = pd.read_csv(ds_metadata_path / \"SAMPLE.csv\", index_col=0)\n",
    "# Subject-level metadata\n",
    "SUBJECT = pd.read_csv(ds_metadata_path / \"SUBJECT.csv\", index_col=0)\n",
    "#  Brain-sample metadata\n",
    "PMDBS = pd.read_csv(ds_metadata_path / \"PMDBS.csv\", index_col=0)\n",
    "# Experimental condition metadata\n",
    "CONDITION = pd.read_csv(ds_metadata_path / \"CONDITION.csv\", index_col=0)\n",
    "\n",
    "# Select Relevant Columns\n",
    "sample_cols = [\n",
    "    \"ASAP_sample_id\",\n",
    "    \"ASAP_subject_id\",\n",
    "    \"ASAP_team_id\",\n",
    "    \"ASAP_dataset_id\",\n",
    "    \"replicate\",\n",
    "    \"condition_id\",\n",
    "]\n",
    "subject_cols = [\n",
    "    \"ASAP_subject_id\",\n",
    "    \"source_subject_id\",\n",
    "    \"sex\",\n",
    "    \"age_at_collection\",\n",
    "    \"primary_diagnosis\",\n",
    "]\n",
    "pmdbs_cols = [\n",
    "    \"ASAP_sample_id\",\n",
    "    \"brain_region\",\n",
    "    \"region_level_1\",\n",
    "    \"region_level_2\",\n",
    "    \"region_level_3\",\n",
    "]\n",
    "condition_cols = [\n",
    "    \"condition_id\",\n",
    "    \"intervention_name\",\n",
    "    \"intervention_id\",\n",
    "    \"protocol_id\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8628c19c",
   "metadata": {},
   "source": [
    "## 5.2 Prepare Metadata Tables\n",
    "\n",
    "We now merge the metadata tables step-by-step.\n",
    "The goal is to construct a single sample-level dataframe (df) that captures all relevant attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c472fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge SAMPLE â†” CONDITION using condition_id\n",
    "df = pd.merge(\n",
    "    SAMPLE[sample_cols], CONDITION[condition_cols], on=\"condition_id\", how=\"left\"\n",
    ")\n",
    "# Merge in SUBJECT information\n",
    "df = pd.merge(df, SUBJECT[subject_cols], on=\"ASAP_subject_id\", how=\"left\")\n",
    "\n",
    "# Merge in brain-region information\n",
    "df = pd.merge(df, PMDBS[pmdbs_cols], on=\"ASAP_sample_id\", how=\"left\")\n",
    "\n",
    "# create unique sample identifier\n",
    "df[\"sample\"] = df[\"ASAP_sample_id\"] + \"_\" + df[\"replicate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d80750",
   "metadata": {},
   "source": [
    "## 5.3 Metadata Cleaning (Fixing Inconsistencies)\n",
    "\n",
    "Before assigning brain region or diagnosis labels to each cell, we correct a few inconsistencies across the original metadata submissions.\n",
    "Different contributing teams may have used slightly different spellings, capitalization, or ontology mappings; this step ensures everything is harmonized for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode brain region to be \"PFC\", \"MFG\", \"HIP\", \"SN\", \"ACG\", \"IPL, \"AMG\", \"PUT\"\n",
    "brain_fix = {\n",
    "    \"Prefrontal Cortex\": \"PFC\",\n",
    "    \"Middle_Frontal_Gyrus\": \"MFG\",\n",
    "    \"Hippocampus\": \"HIP\",\n",
    "    \"Substantia_Nigra \": \"SN\",\n",
    "    \"Substantia_Nigra\": \"SN\",\n",
    "    \"ACG\": \"ACG\",\n",
    "    \"IPL\": \"IPL\",\n",
    "    \"Middle temporal gyrus\": \"MTG\",\n",
    "    \"Substantia nigra\": \"SN\",\n",
    "    \"Prefrontal cortex\": \"PFC\",\n",
    "    \"Amygdala\": \"AMG\",\n",
    "    \"Putamen\": \"PUT\",\n",
    "}\n",
    "df[\"brain_region\"] = df[\"brain_region\"].map(brain_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now map to find more course designations\n",
    "brain_simple = {\n",
    "    \"PFC\": \"frontal_ctx\",\n",
    "    \"MFG\": \"frontal_ctx\",\n",
    "    \"ACG\": \"cingulate_ctx\",\n",
    "    \"IPL\": \"parietal_ctx\",\n",
    "    \"MTG\": \"temporal_ctx\",\n",
    "    \"HIP\": \"subcortical\",\n",
    "    \"AMG\": \"subcortical\",\n",
    "    \"PUT\": \"subcortical\",\n",
    "    \"SN\": \"subcortical\",\n",
    "}\n",
    "\n",
    "df[\"brain_region_simple\"] = df[\"brain_region\"].map(brain_simple)\n",
    "\n",
    "\n",
    "# define sample to match\n",
    "br_mapper_full = dict(zip(df[\"sample\"], df[\"brain_region\"]))\n",
    "br_mapper_simple = dict(zip(df[\"sample\"], df[\"brain_region\"].map(brain_simple)))\n",
    "\n",
    "# Parkinsons and control samples\n",
    "condition_id_mapper = dict(zip(df[\"sample\"], df[\"condition_id\"]))\n",
    "case_id_mapper = dict(zip(df[\"sample\"], df[\"intervention_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b038366",
   "metadata": {},
   "source": [
    "## 5.4 Save Merged Metadata\n",
    "\n",
    "Now that the dataset-level metadata is assembled, we save it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765bebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metadata_filen = local_data_path / \"asap-cohort-dataset-metadata.csv\"\n",
    "df.to_csv(dataset_metadata_filen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASAP-CRN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
